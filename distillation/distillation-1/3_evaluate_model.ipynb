{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a43d3b6-b87a-49a2-92c0-85fee4e6926e",
   "metadata": {},
   "source": [
    "# Introduction to Model Distillation: Efficient Knowledge Transfer for AI Applications - Part 3\n",
    "\n",
    "So far we have:\n",
    "\n",
    "- created synthetic data set (Using [1_generate_synthetic_data.ipynb](1_generate_synthetic_data.ipynb))\n",
    "- fine tuned a custom model (Using [2_fine_tuning.ipynb](2_fine_tuning.ipynb))\n",
    "\n",
    "This notebook shows how to test the new model\n",
    "\n",
    "\n",
    "## Pre requisites\n",
    "\n",
    "- Run [1_generate_synthetic_data.ipynb](1_generate_synthetic_data.ipynb)\n",
    "- Run [2_fine_tuning.ipynb](2_fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8fe8e",
   "metadata": {},
   "source": [
    "## 1 - Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08124040-bef8-47cf-8fb7-5860ede4b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Sequence\n",
    "from openai import Client\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8551e9",
   "metadata": {},
   "source": [
    "## 2 - Configuration\n",
    "\n",
    "Get model name in AI Studio\n",
    "\n",
    "![](distilled-model-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73c9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# read model name from file\n",
    "model_name_file = 'model_name.out'\n",
    "if os.path.exists(model_name_file):\n",
    "    with open(model_name_file, 'r') as f:\n",
    "        MODEL_NAME = f.read().strip()\n",
    "else:\n",
    "    # fallback to default model name if file does not exist\n",
    "    raise Exception (f\"Model name file '{model_name_file}' not found. Please run the fine-tuning notebook (2_fine_tuning.ipynb) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66993377",
   "metadata": {},
   "source": [
    "## 3  - Initialize Client\n",
    "\n",
    "The cell below creates the OpenAI-like Client to work with Nebius AI Studio and defines necessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6a4dba-e97b-4e0c-82b0-225758ec6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_CACHE_DIR = 'cache'\n",
    "BASE_URL = \"https://api.studio.nebius.ai\"\n",
    "\n",
    "client = Client(\n",
    "    base_url=f'{BASE_URL}/v1',\n",
    "    api_key=os.getenv('NEBIUS_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488131f6-5ce5-4515-a1fd-8ccae9bd4a0f",
   "metadata": {},
   "source": [
    "## 4 - Test our model\n",
    "\n",
    "Let's test our model on an example.\n",
    "\n",
    "**Original sentence**: \"Nebius AI Studio is a comprehensive platform designed to simplify the integration of AI capabilities into applications.\"\n",
    "\n",
    "**Modified sentence**: \"Nebius AI Studio is **~~a~~** comprehensive platform designed too simplify **~~the~~** integration of AI **capabiltes** into **applcations**.\"\n",
    "\n",
    "The errors introduced are highlighted in **bold**.\n",
    "\n",
    "First, generate the corrected version using our trained model and compare it with the original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79cf0fbe-0ad2-40d2-990e-f7ccfde423bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model generation coincides with the original text!\n"
     ]
    }
   ],
   "source": [
    "system_prompt_fine_tuning = \"Please correct the grammar in the user's text if necessary.\"\n",
    "\n",
    "empty_reasoning_prefix = \"\"\"\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "\n",
    "\"\"\".lstrip()\n",
    "\n",
    "sample_text = \"Nebius AI Studio is a comprehensive platform designed to simplify the integration of AI capabilities into applications.\"\n",
    "text_with_errors = \"Nebius AI Studio is comprehensive platform designed too simplify integration of AI capabiltes into applcations.\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': system_prompt_fine_tuning},\n",
    "        {'role': 'user', 'content': text_with_errors}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    max_tokens=40\n",
    ")\n",
    "model_generation = resp.choices[0].message.content.split('</think>')[-1].strip()\n",
    "\n",
    "if sample_text == model_generation:\n",
    "    print('Model generation coincides with the original text!')\n",
    "else:\n",
    "    print('Model generation differs from the original text:', model_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f5fc6-3e6f-4571-9cf0-f41088290342",
   "metadata": {},
   "source": [
    "We can see that the generation coincides with the original text, which means our model did its job! Let's now evaluate the quality of our fine-tuned model and ensure it provides superior quality compared to a baseline model - Qwen3-14B - a larger model from the same family. Again, we can do this conveniently using Nebius AI Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fe29f-1b4b-47b4-98e2-303f34c1a345",
   "metadata": {},
   "source": [
    "## 5 - Evaluate the model\n",
    "\n",
    "_Heads up: Running this part will cost ~\\\\$3.4 (generations with the models cost ~ \\\\$0.02). Changing the evaluation model to its non-reasoning version DeepSeek-V3 will reduce the cost of running the model to ~\\\\$0.5, but can slightly decrease the reliability of evaluation._\n",
    "\n",
    "The evaluation procedure in text generation tasks is not as straightforward as in classification tasks. This is because the task can have multiple correct outputs for the given input. For example, in our task, in the input text \"The team leader which I spoke to yesterday, ...\", the word \"which\" can be corrected to \"who\" and \"whom\", and both options are correct.\n",
    "\n",
    "Therefore, the procedure of evaluation highly depends on the nature of the task. In some cases, for example, story generation, one would have to ask an LLM to verify every generated story against the input data. For other tasks like sentence paraphrasing, some generated paraphrases may coincide with the reference ones, removing the need to run LLM on these coinciding cases. Still, for almost any text generation task, some cases need to be evaluated either with a human's or an LLM's help. We can again leverage Nebius AI Studio to run the evaluator LLM.\n",
    "\n",
    "We will take the test set of the JFLEG dataset[4], used for evaluation of grammatic error correction systems. Each instance is a sentence that contains 4 corrections from 4 language experts. Some corrections may coincide with the original sentence if the sentence is grammatically correct. The dataset contains spaces before the punctuation marks - we'll remove them to have the data in human-like format.\n",
    "\n",
    "For our task, grammatic error correction, we can also leverage its peculiarities to simplify the evaluation and reduce the number of cases where the results depend on another LLM's verdict.\n",
    "We will use the following pipeline. After we generate corrections with the fine-tuned and baseline models, we will compare these corrections with the reference corrections. If a model generation coincides with at least one expert correction, we will consider it valid. If it doesn't, and the model generation introduces no edits to the original text, this means an error since all the experts suggested corrections. Finally, if none of the conditions above is met, we will ask a powerful LLM to assess whether the correction suggested by the model is correct or not.\n",
    "\n",
    "Because the baseline model is not that large (14B parameters) and it was not trained to generate data in the desired format, it can sometimes fail to strictly follow the output format. Hence, we'll use extended instructions with few-shot learning examples to guide it to output the results in the desired format - the system prompt we used to distill Qwen/Qwen3-235B-A22B.\n",
    "\n",
    "As already discussed, we will use both models in non-reasoning mode to simulate their real-world application where the result should be generated on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243b236-9da8-425b-af22-0bcee8c08881",
   "metadata": {},
   "source": [
    "### 5.1 - Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0d1442-b5bf-4322-aaa9-ee3f842c1d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546e113ed3f34aafb8fcc3024434be25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910c1833b1d346b19d3396c4a45fb284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a58a72530ef4a97a8fb36e49cd0e2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'New and new technology has been introduced to the society .', 'corrections': ['New technology has been introduced to society .', 'New technology has been introduced into the society .', 'Newer and newer technology has been introduced into society .', 'Newer and newer technology has been introduced to the society .']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'corrections'],\n",
       "    num_rows: 747\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = load_dataset('jhu-clsp/jfleg', cache_dir=DATASETS_CACHE_DIR)['test']\n",
    "# Remove bad instances where no corrections are suggested or the sentence is empty\n",
    "eval_dataset = eval_dataset.filter(lambda x: x['sentence'] != '' and not all(y == '' for y in x['corrections']))\n",
    "print(eval_dataset[0])\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a67f2-d512-49ef-b67f-d98f0c63b5b3",
   "metadata": {},
   "source": [
    "### 5.2 - Preprocess it by removing spaces before punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301d6632-8a8d-46d3-a2bf-f22111a88b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5032ed35e442d4b490f3869bac45b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a73ea54a2bd4f6b91c991932a3322c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'corrections'],\n",
       "    num_rows: 747\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_spacing(text):\n",
    "    return re.sub(r'\\s+([.,:;?!])', r'\\1', text).strip()\n",
    "\n",
    "eval_dataset = eval_dataset.map(lambda x: {\"sentence\": fix_spacing(x[\"sentence\"])}, remove_columns=\"sentence\")\n",
    "eval_dataset = eval_dataset.map(lambda x: {\"corrections\": list(map(fix_spacing, x[\"corrections\"]))}, remove_columns=\"corrections\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5be729-79d0-4d5c-8cdd-911b1295cc35",
   "metadata": {},
   "source": [
    "## 6 - Create answers\n",
    "\n",
    "Now that we have the data, let's generate results with both models. To diversify our tutorial, let's use normal synchronous generation here. We can parallelize it to accelerate the process.\n",
    "\n",
    "We have two choices to enable non-reasoning mode:\n",
    "- force the models to start generation with the empty reasoning pattern - this is how the authors of Qwen3 suggest doing\n",
    "- add the \"/no_think\" token to the user's message\n",
    "\n",
    "Since the \"/no_think\" token will anyway urge the model to start the generation with the empty reasoning pattern, let's use the first option. We'll need to adopt two additional arguments for the completion endpoint of AI Studio - `continue_final_message` and `add_generation_prompt`. The first argument formats the chat so that the final message in the chat is open-ended, without any EOS tokens. This enables the model to continue the final message rather than starting a new one. When it is set to `True`, the second argument must be set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f72be8-556e-4e31-9efa-6aa419b3044b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [02:01<00:00,  6.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<think>\\nOkay, let\\'s see. The user wrote: \"New and new technology has been introduced to the society.\" The user wants me to correct the grammar if necessary.\\n\\nFirst, I need to check if there are any grammatical errors. The phrase \"New and new technology\" seems repetitive. The word \"new\" is used twice. That\\'s probably a redundancy. So maybe it should be \"New technology\" instead of \"New and new technology.\" \\n\\nNext, the verb \"has been introduced\" is correct, but the subject \"New and new technology\" is singular. Wait, but \"technology\" is uncountable,',\n",
       " '<think>\\nOkay, the user provided a sentence and wants me to correct any grammar issues. Let me read through it carefully.\\n\\nThe original sentence: \"One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries.\"\\n\\nFirst, I need to check for grammatical errors. Let\\'s start with the structure. The sentence is complex, so I should ensure that the clauses are correctly connected. The main clause is \"One possible outcome is that...\" followed by a dependent clause. The dependent clause uses \"that\" correctly.\\n\\nLooking at the words: \"environmentally']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def _call_llm(input_text: str, system_prompt: str, **call_kwargs):\n",
    "    return client.chat.completions.create(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': input_text},\n",
    "            {'role': 'assistant', 'content': empty_reasoning_prefix}\n",
    "        ],\n",
    "        extra_body={\"continue_final_message\": True, \"add_generation_prompt\": False},\n",
    "        **call_kwargs\n",
    "    ).choices[0].message.content\n",
    "\n",
    "def generate(\n",
    "    input_text: str,\n",
    "    model: str = MODEL_NAME,\n",
    "    system_prompt: str = system_prompt_fine_tuning,\n",
    "    max_tokens: int = 128,  # setting to 128 as default to make sure our generation won't terminate on long sentences\n",
    "    top_p: float = 0.8,  # as suggested by Qwen3 authors\n",
    "    temperature: float = 0.7,  # as suggested by Qwen3 authors\n",
    ") -> str:\n",
    "    # Use try-except construction in case we encounter an error \n",
    "    output = None\n",
    "    while output is None:\n",
    "        try:\n",
    "            return _call_llm(\n",
    "                input_text=input_text,\n",
    "                system_prompt=system_prompt,\n",
    "                model=model,\n",
    "                max_tokens=max_tokens,\n",
    "                top_p=top_p\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f'Error for input text {input_text}:\\n{e}\\nSleeping for 3 seconds and trying again...')\n",
    "            time.sleep(3)\n",
    "            continue\n",
    "        \n",
    "ft_model_generations = [None] * len(eval_dataset)\n",
    "# Be careful with quota limits - you may need to reduce it\n",
    "max_workers = 12\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(generate, inst['sentence'], MODEL_NAME, system_prompt_fine_tuning): idx for idx, inst in enumerate(eval_dataset)}\n",
    "    for future in tqdm(as_completed(futures), total=len(eval_dataset)):\n",
    "        idx = futures[future]\n",
    "        ft_model_generations[idx] = future.result()\n",
    "ft_model_generations[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de3e6f-ec60-4755-9ce8-b372e2b997a6",
   "metadata": {},
   "source": [
    "Let's repeat the procedure for the baseline model. We need to change the prompt (as discussed earlier) and reduce the number of parallelization workers in order not to exceed the quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4226e41c-663e-4e9f-bc75-a68e32286944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [01:36<00:00,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.04 s, sys: 318 ms, total: 5.36 s\n",
      "Wall time: 1min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<think>\\nOkay, let\\'s take a look at the user\\'s sentence: \"New and new technology has been introduced to the society.\" The user wants me to check the grammar. \\n\\nFirst, I notice \"New and new technology.\" That repetition of \"new\" seems off. Maybe they meant \"New and emerging technology\" or \"New and innovative technology.\" But the user might just be emphasizing the novelty, so maybe \"new\" is intentional. However, in standard grammar, repeating the same adjective isn\\'t typically correct unless for stylistic reasons. But the user\\'s example shows they correct repetition, so I should check that.\\n\\nNext,',\n",
       " '<think>\\nOkay, let me take a look at the user\\'s sentence. The original text is: \"One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries.\"\\n\\nFirst, I need to check for grammar errors. The sentence structure seems correct. The main clause is \"One possible outcome is that...\" followed by a subordinate clause. The use of \"will outweigh\" is appropriate here because it\\'s a future prediction. \\n\\nLooking at the adjectives: \"richer countries\" and \"poorer countries\" are correctly comparative forms. The phrase \"environment']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "system_prompt_distillation = \"\"\"\n",
    "Act as an experienced English proofreader. Please check the grammar of the user's text. If the text contains errors or misprints, print the corrected text. Otherwise, print the text as it is, otherwise. Check only the grammar of the text. Don't print anything else.\n",
    "\n",
    "Examples for few-shot learning:\n",
    "Example 1 (the text contains errors):\n",
    "User: In fact who let me know abut this program was him.\n",
    "Assistant: In fact, he was the one who let me know about this program.\n",
    "\n",
    "Example 2 (the text does not contain errors):\n",
    "User: On the other hand, it's very efficient computationally as it only requires one forward pass through the model per example.\n",
    "Assistant: On the other hand, it's very efficient computationally as it only requires one forward pass through the model per example.\n",
    "\"\"\".strip()\n",
    "\n",
    "bs_model_generations = [None] * len(eval_dataset)\n",
    "# Reduce the parallelization to prevent rate limit errors\n",
    "max_workers = 16\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:  \n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            generate,\n",
    "            inst['sentence'],\n",
    "            'Qwen/Qwen3-14B',\n",
    "            system_prompt_distillation,\n",
    "        ): idx\n",
    "        for idx, inst in enumerate(eval_dataset)\n",
    "    }\n",
    "    for future in tqdm(as_completed(futures), total=len(eval_dataset)):\n",
    "        idx = futures[future]\n",
    "        bs_model_generations[idx] = future.result()\n",
    "bs_model_generations[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd0089-120c-4b13-be7a-592eeb421a87",
   "metadata": {},
   "source": [
    "## 7 - Which answers need LLM verification?\n",
    "\n",
    "Now let's determine cases where an LLM is required to evaluate the generation of the model. As discussed, this means cases where the model generation differs from all corrections and the original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a2498-27f8-4ce8-902f-6aece899195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.5 ms, sys: 2.03 ms, total: 39.5 ms\n",
      "Wall time: 38.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(747, 747)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_results = {}\n",
    "bs_results = {}\n",
    "ft_ids_require_verification = []\n",
    "bs_ids_require_verification = []\n",
    "coinciding_ids = set()\n",
    "\n",
    "for i in range(len(eval_dataset)):\n",
    "    row = eval_dataset[i]\n",
    "    sentence = row['sentence']\n",
    "    corrections = row['corrections']\n",
    "    # If generation is in corrections, it is considered valid\n",
    "    if ft_model_generations[i] in corrections:\n",
    "        ft_results[i] = 1\n",
    "    # Else, if generation coincides with the original sentence, it is considered \n",
    "    # invalid  because the original sentence is not among the corrections\n",
    "    # (otherwise it would meet the previous condition)\n",
    "    elif ft_model_generations[i] == sentence:\n",
    "        ft_results[i] = 0\n",
    "    # Otherwise need to call an LLM for verification\n",
    "    else:\n",
    "        ft_ids_require_verification.append(i)\n",
    "\n",
    "    # Same for the baseline model\n",
    "    if bs_model_generations[i] in corrections:\n",
    "        bs_results[i] = 1\n",
    "    elif bs_model_generations[i] == sentence:\n",
    "        bs_results[i] = 0\n",
    "    # If the generation doesn't fall under the two conditions and coincides with the fine-tuned model generation,\n",
    "    # we don't need to duplicate verification for the baseline model\n",
    "    elif bs_model_generations[i] == ft_model_generations[i]:\n",
    "        coinciding_ids.add(i)\n",
    "    else:\n",
    "        bs_ids_require_verification.append(i)\n",
    "len(ft_ids_require_verification), len(bs_ids_require_verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ad9e2-c371-46a5-9308-3d2db8e2c17e",
   "metadata": {},
   "source": [
    "## 8- Using LLM for verification\n",
    "\n",
    "Verification of generations is not a trivial task and requires some reasoning from the model. Furthermore, to avoid correlation of scores with the generations, \n",
    "\n",
    "* let's use another powerful reasoning LLMs here - **DeepSeek-R1**. We will use its 'fast' checkpoint to speed up the process.\n",
    "\n",
    "* Our fine-tuned model was trained on data produced by **Qwen/Qwen3-235B-A22B**, while the baseline model also comes from the Qwen3 family. Using **Qwen/Qwen3-235B-A22B** for evaluation in this scenario may lead to overrated scores since this model's generations correlate with generations of the two models we are evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f374e4cb-4617-4d34-9267-bce57d032991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 57/747 [00:38<06:08,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence \"And what are those risks?\" is grammatically correct. All four experts agree that no changes are needed, and the proofreader's correction matches the original. Therefore, the proofreader's correction is valid.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 107/747 [01:20<06:35,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 146/747 [01:48<07:02,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence has several errors: \"yonger\" is misspelled as \"younger,\" the verb \"need\" should be \"needs\" for third-person singular, and the phrase \"need to more than\" is grammatically incorrect. The proofreader's correction addresses all these issues by changing \"yonger\" to \"younger,\" \"need\" to \"needs,\" and removing the unnecessary \"to.\" The corrected sentence matches the valid corrections provided by the experts (options 0, 2, 3), making the proofreader's correction valid.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 150/747 [01:49<03:58,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 230/747 [02:41<05:40,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 244/747 [02:53<04:48,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence \"So what shall I do?\" is grammatically correct. All four experts agreed that no correction was needed. The proofreader's correction matches the original, indicating they correctly identified no errors. Therefore, the proofreader's correction is valid.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 281/747 [03:22<04:38,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence has two errors: \"strengthen\" should be \"strengthened\" (passive voice with \"will be\"), and \"how\" should be \"who\" (referring to people/producers). The proofreader's correction addresses both errors. The experts' corrections also fix these issues, using either \"who\" or \"that\" (both acceptable here). The proofreader's version matches correction 0, which is valid.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 313/747 [03:39<03:25,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence is grammatically correct. All four experts agree that no changes are needed. The proofreader's correction matches the original, so it's valid.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 609/747 [06:08<01:35,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. The original sentence \"Everybody knows each other.\" is grammatically correct. All four experts agree that no changes are needed. The proofreader's correction matches the original, so it's valid.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 626/747 [06:15<00:42,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 641/747 [06:23<01:09,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence has two main issues: the misspelling \"pupluation\" (corrected to \"population\") and the awkward/inaccurate use of \"outburst\" (corrected to \"explosion\"). The proofreader's correction fixes both errors. The experts' corrections also address these points (e.g., \"population\" and \"explosion\" in correction 3). The proofreader's version aligns with valid corrections, making it accurate.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 663/747 [06:32<00:24,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer The proofreader's correction fails to address several errors present in the original sentence and introduces new issues:\n",
      "\n",
      "1. **Subject-verb agreement**: The original error \"points is\" (plural subject + singular verb) remains uncorrected as \"points is\" instead of \"point is\".\n",
      "\n",
      "2. **Article omission**: \"according the the passage\" is corrected to \"according to the passage\", but the double \"the\" remains (\"the the passage\").\n",
      "\n",
      "3. **Spelling error**: \"know Old Kindgom King\" remains uncorrected (should be \"known Old Kingdom King\").\n",
      "\n",
      "4. **Verb tense inconsistency**: \"carries and identifies\" remains unchanged despite context requiring past tense.\n",
      "\n",
      "5. **Capitalization error**: \"khafre\" at the end remains uncorrected to proper noun capitalization.\n",
      "\n",
      "6. **New errors introduced**: The proofreader adds incorrect punctuation (\"but the lecture says that, these evidence\") and fails to fix the ungrammatical \"these evidence\" (should be \"this evidence\").\n",
      "\n",
      "Since the proofreader's version retains multiple grammatical errors from the original and introduces new issues while failing to incorporate valid corrections from the expert suggestions, the correction is invalid.\n",
      "\n",
      "Answer: 0. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [07:24<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.16 s, sys: 594 ms, total: 6.75 s\n",
      "Wall time: 7min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "evaluation_model = 'deepseek-ai/DeepSeek-R1-fast'\n",
    "\n",
    "system_prompt_evaluate = \"\"\"\n",
    "Act as an experienced grammar checker.\n",
    "\n",
    "You will be provided with:\n",
    "1. A sentece that may contain errors\n",
    "2. Its 4 corrections suggested by 4 language experts (corrections may coincide with the sentence if the expert believes the sentence is grammatically correct)\n",
    "3. Its correction suggested by the proofreader\n",
    "\n",
    "Please evaluate whether the correction written by a proofreader is valid (1) or not (0). \\\n",
    "The correction is considered invalid if it omits editing any of the errors or rectifies indeed gramatically correct pieces of text. \\\n",
    "Otherwise, it is considered valid.\n",
    "\n",
    "Again, please output:\n",
    "- \"1\", if the editing suggested by the proofreader is correct\n",
    "- \"0\", if the editing suggested by the proofreader is incorrect\n",
    "\n",
    "Only output the number (\"0\" / \"1\") and nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "user_prompt_evaluate_template = \"\"\"\n",
    "Sentence:\n",
    "{sentence}\n",
    "\n",
    "Corrections:\n",
    "{corrections}\n",
    "\n",
    "Proofreader's correction:\n",
    "{generation}\n",
    "\"\"\".strip()\n",
    "\n",
    "def evaluate_text(\n",
    "    row: dict[str, str | list[str]], generation: str, system_prompt: str = system_prompt_evaluate\n",
    ") -> str:\n",
    "    formatted_corrections = '\\n'.join(f\"{i}. {correction}\" for i, correction in enumerate(row['corrections']))    \n",
    "    user_prompt = str(user_prompt_evaluate_template).format(\n",
    "        sentence=row['sentence'],\n",
    "        corrections=formatted_corrections,\n",
    "        generation=generation\n",
    "    )\n",
    "    answer_with_reasoning = client.chat.completions.create(\n",
    "        model=evaluation_model,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt_evaluate},\n",
    "            {'role': 'user', 'content': user_prompt},\n",
    "        ],\n",
    "        max_tokens=4096,  # to make sure the model finishes the reasoning and outputs an answer\n",
    "        top_p=0.01,  # to make the results as deterministic as possible\n",
    "    ).choices[0].message.content\n",
    "    answer = answer_with_reasoning.split('</think>')[-1].strip()\n",
    "    return convert_verdict_to_number(answer)\n",
    "\n",
    "def convert_verdict_to_number(verdict: str) -> int:\n",
    "    if verdict.isdigit():\n",
    "        return int(verdict)\n",
    "    # To make sure we don't overrate the performance of the model, if DeepSeek-R1 outputs smth else\n",
    "    # than a number, consider the editing of the model invalid\n",
    "    print(f\"Cannot parse the answer {verdict}. Replacing with 0.\")\n",
    "    return 0\n",
    "\n",
    "def evaluate(ids_require_verification: Sequence[int], model_generations: list[str]) -> str:\n",
    "    data_require_verification = eval_dataset.select(ids_require_verification)\n",
    "    model_labels = [None] * len(ids_require_verification)\n",
    "    max_workers = 16\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:  # Reduce the parallelization by 8 times\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                evaluate_text,\n",
    "                data_require_verification[idx],\n",
    "                generation\n",
    "            ): idx\n",
    "            for idx, generation in enumerate(\n",
    "                [model_generations[x] for x in ids_require_verification]\n",
    "            )\n",
    "        }\n",
    "        for future in tqdm(as_completed(futures), total=len(ids_require_verification)):\n",
    "            idx = futures[future]\n",
    "            model_labels[idx] = future.result()\n",
    "    return np.array(model_labels)\n",
    "\n",
    "ft_verif_labels = evaluate(ft_ids_require_verification, ft_model_generations)\n",
    "ft_verif_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83069910-47cd-42c9-94b2-5bf6ea7559bb",
   "metadata": {},
   "source": [
    "Let's do the same staff for the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230dce77-42bf-4bc8-9a63-e6159e51794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 145/747 [01:21<04:47,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 184/747 [01:43<03:21,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence \"I think it will be lost.\" is grammatically correct. All four experts agree that no correction is needed. The proofreader's correction matches the original, confirming its validity. Since there are no errors to address, the proofreader's response is correct.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 235/747 [02:08<02:53,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 270/747 [02:25<05:30,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence \"It can certainly be the case.\" is grammatically correct. All four experts agreed that no changes were needed. The proofreader's correction matches the original, indicating they found no errors. Since there are no mistakes to correct, the proofreader's response is valid.\n",
      "\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 440/747 [03:56<02:02,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence uses \"creative\" (adjective) and \"innovation\" (noun), which are mismatched. The correct versions should pair either both nouns (\"creativity and innovation\") or both adjectives (\"creative and innovative\"). The proofreader's correction changes \"creative\" to \"creativity\" (noun) to match \"innovation\" (noun), which aligns with the experts' corrections. Therefore, the proofreader's edit is valid.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 485/747 [04:20<02:24,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 557/747 [04:52<01:14,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "0. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 560/747 [04:54<01:36,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1\n",
      "\n",
      "The original sentence uses \"think\" instead of the past participle \"thought\" required by the present perfect tense (\"Have you ever...\"). All four experts agree on changing \"think\" to \"thought,\" and the proofreader made this correction. The proofreader's edit addresses the only grammatical error, making the correction valid.. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 610/747 [05:15<00:36,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 712/747 [06:07<00:14,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse the answer <think>\n",
      "1. Replacing with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [06:28<00:00,  1.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_verif_labels = evaluate(bs_ids_require_verification, bs_model_generations)\n",
    "bs_verif_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051105d-8c14-4641-b4c6-0c42195e70d9",
   "metadata": {},
   "source": [
    "## 9- Measure accuracy of answers\n",
    "\n",
    "Let's print the accuracy, build 95% confidence intervals, and see how our fine-tuned model performs against the baseline. Since the number of observations is quite large (747) and $p$ (observed accuracy) is not close to 0 or 1, we can use the normal approximation for a binomial proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "873b1771-6884-4f49-98ac-fabe9924a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model:\n",
      "95% confidence interval: 0.838 ± 0.026\tLeft boundary: 0.812\tRight boundary: 0.864\n",
      "Baseline model:\n",
      "95% confidence interval: 0.896 ± 0.022\tLeft boundary: 0.874\tRight boundary: 0.918\n",
      "CPU times: user 667 μs, sys: 0 ns, total: 667 μs\n",
      "Wall time: 637 μs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def print_confidence_interval(p, num_obs):\n",
    "    # Critical value for 95% CI\n",
    "    z = 1.96\n",
    "    deviation = z * ((p * (1 - p) / num_obs) ** .5)\n",
    "    left_boundary = p - deviation\n",
    "    right_boundary = p + deviation\n",
    "    print(f'95% confidence interval: {p:.3f} ± {deviation:.3f}\\tLeft boundary: {left_boundary:.3f}\\tRight boundary: {right_boundary:.3f}')\n",
    "\n",
    "# Fine-tuned model\n",
    "ft_verif_dict = {idx: label for idx, label in zip(ft_ids_require_verification, ft_verif_labels)}\n",
    "ft_results.update(ft_verif_dict)\n",
    "ft_accuracy = np.mean(list(ft_results.values()))\n",
    "print('Fine-tuned model:')\n",
    "print_confidence_interval(ft_accuracy, len(ft_results))\n",
    "\n",
    "# Baseline model\n",
    "bs_verif_dict = {idx: label for idx, label in zip(bs_ids_require_verification, bs_verif_labels)}\n",
    "bs_results.update(bs_verif_dict)\n",
    "# Coinciding ids\n",
    "for idx in coinciding_ids:\n",
    "    bs_results[idx] = ft_verif_dict[idx]\n",
    "bs_accuracy = np.mean(list(bs_results.values()))\n",
    "print('Baseline model:')\n",
    "print_confidence_interval(bs_accuracy, len(bs_results))\n",
    "\n",
    "assert len(ft_results) == len(bs_results) == len(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ee72a-57d8-4910-953e-478c60dab2b4",
   "metadata": {},
   "source": [
    "We can see the fine-tuned model slightly outperforms the baseline model on average with the scores staying within the confidence interval of each other. On the other side, the fine-tuned model works 2.5x times faster and reduces the token consumption due to the possibility of using shorter prompts, making the model both more efficient and effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d164612-9381-41e4-9eb2-a208b9a65d54",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370287c-0c2c-493d-a1d0-898446e0f780",
   "metadata": {},
   "source": [
    "In this tutorial, we've demonstrated the power of model distillation using Nebius AI Studio. By leveraging Qwen3-235B-A22B as our teacher model and fine-tuning Qwen3-4B as our student, we've created a grammar correction model that performs on par with a larger baseline Qwen3-14B model while being 3.5x times smaller and requiring less computational resources at inference time.\n",
    "\n",
    "Nebius AI Studio streamlines the entire distillation workflow - from generating high-quality training data with powerful teacher models to fine-tuning efficient student models and seamlessly deploying them for inference. The end-to-end platform makes advanced AI techniques accessible even without extensive infrastructure or expertise.\n",
    "Start leveraging the power of model distillation for your own applications today and experience how Nebius AI Studio can help you achieve large capabilities in small packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb6965-5900-4f7c-b578-af1012381256",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Stahlberg, F., & Kumar, S. (2021). Synthetic data generation for grammatical error correction with tagged corruption models. In Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications (pp. 37–47). Association for Computational Linguistics. https://www.aclweb.org/anthology/2021.bea-1.4\n",
    "2. Qwen Team. (2025, April 29). *Qwen3: Think deeper, act faster*. Qwen Blog. https://qwenlm.github.io/blog/qwen3/\n",
    "3. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. https://openreview.net/forum?id=nZeVKeeFYf9\n",
    "4. Napoles, C., Sakaguchi, K., & Tetreault, J. (2017). JFLEG: A fluency corpus and benchmark for grammatical error correction. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers (pp. 229–234). Association for Computational Linguistics. http://www.aclweb.org/anthology/E17-2037"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distillation-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
