{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izmeeJoOrkTt"
      },
      "source": [
        "# Run GPT-OSS model on Nebius AI Studio\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nebius/ai-studio-cookbook/blob/main/models/gpt_oss_1.ipynb)\n",
        "[![](https://img.shields.io/badge/Powered%20by-Nebius-orange?style=flat&labelColor=darkblue&color=orange)](https://nebius.com/ai-studio)\n",
        "\n",
        "GPT OSS is a hugely anticipated open-weights release by OpenAI, designed for powerful reasoning, agentic tasks, and versatile developer use cases.  It comes in 2 sizes:\n",
        "- a big one with 117B parameters (gpt-oss-120b),\n",
        "- and a smaller one with 21B parameters (gpt-oss-20b).\n",
        "\n",
        "Both are mixture-of-experts (MoEs) and use a 4-bit quantization scheme (MXFP4)\n",
        "\n",
        "[Read more here](https://github.com/nebius/ai-studio-cookbook/blob/main/models/gpt-oss.md)\n",
        "\n",
        "## References\n",
        "\n",
        "- [OpenAI open models page](https://openai.com/open-models/)  |  [github   openai/gpt-oss](https://github.com/openai/gpt-oss)\n",
        "- Model card for 120B param model: [gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b)\n",
        "- Model card for smaller 20B param model: [gpt-oss-20b](https://hf.co/openai/gpt-oss-20b)\n",
        "- [Nebius AI Studio](https://studio.nebius.com/)\n",
        "\n",
        "## Pre requisites\n",
        "\n",
        "- Nebius API key.  Sign up for free at [AI Studio](https://studio.nebius.com/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3KSn4LIvFo0"
      },
      "source": [
        "## 1 - Getting Started\n",
        "\n",
        "### 1.1 - Get your Nebius API key at [Nebius AI Studio](https://studio.nebius.com/)\n",
        "\n",
        "### 1.2 - If running on Google Colab ...\n",
        "\n",
        "Add `NEBIUS_API_KEY` to **Secrets** panel on the left as follows\n",
        "\n",
        "![](https://github.com/nebius/ai-studio-cookbook/raw/main/images/google-colab-1.png)\n",
        "\n",
        "\n",
        "### 1.3 - If running locally\n",
        "\n",
        "Create an `.env` file with NEBIUS_API_KEY as follows\n",
        "\n",
        "```text\n",
        "NEBIUS_API_KEY=your_api_key_goes_here\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNJ-WCDSEy0G"
      },
      "source": [
        "## 2 - Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OTElVQApE9I7"
      },
      "outputs": [],
      "source": [
        "%pip install -q openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp6woeSsFGo5"
      },
      "source": [
        "## 2 - Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFSX16IeFLKa",
        "outputId": "63a7312a-7f25-4fd3-9d43-edb720dc480e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Colab\n",
            "‚úÖ NEBIUS_API_KEY found\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "\n",
        "## Recommended way of getting configuration\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   print(\"Running in Colab\")\n",
        "   from google.colab import userdata\n",
        "   NEBIUS_API_KEY = userdata.get('NEBIUS_API_KEY')\n",
        "else:\n",
        "   print(\"NOT running in Colab\")\n",
        "\n",
        "   from dotenv import load_dotenv\n",
        "   load_dotenv()\n",
        "   NEBIUS_API_KEY = os.getenv('NEBIUS_API_KEY')\n",
        "\n",
        "\n",
        "## quick hack (not recommended) - you can hardcode the config key here\n",
        "# NEBIUS_API_KEY = \"your_key_here\"\n",
        "\n",
        "if NEBIUS_API_KEY:\n",
        "  print ('‚úÖ NEBIUS_API_KEY found')\n",
        "  os.environ['NEBIUS_API_KEY'] = NEBIUS_API_KEY\n",
        "else:\n",
        "  raise RuntimeError ('‚ùå NEBIUS_API_KEY NOT found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smYPQAlBtgTQ"
      },
      "source": [
        "## 3 - Run the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67AElPbzP7z"
      },
      "source": [
        "### 3.1 - Initialize the client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bx3BAsiIxfqi"
      },
      "outputs": [],
      "source": [
        "## Create a client\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
        "    api_key=os.environ.get('NEBIUS_API_KEY')\n",
        ")\n",
        "\n",
        "## Select a model\n",
        "MODEL_NAME = \"openai/gpt-oss-120b\" # big model\n",
        "#MODEL_NAME = \"openai/gpt-oss-20b\"  # small brother"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbKjxJndzT6r"
      },
      "source": [
        "### 3.2 - Find out the model's capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1gN-e3Iw9H3",
        "outputId": "efcf1c46-7db6-4b1c-e06f-59c0c4008af1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----model answer -----\n",
            "I‚Äôm a GPT‚Äë4‚Äëbased AI assistant, so my main strength is working with natural language (and, when an image is supplied, visual information as well). Here‚Äôs a quick rundown of what I can do:\n",
            "\n",
            "| Category | What I can help with |\n",
            "|----------|----------------------|\n",
            "| **Information & Knowledge** | Answer factual questions, explain concepts, summarize articles, compare topics, and provide background on science, history, technology, arts, etc. (knowledge up to‚ÄØJune‚ÄØ2024). |\n",
            "| **Writing & Creativity** | Draft essays, reports, emails, blog posts, stories, poems, jokes, dialogue, scripts, marketing copy, social‚Äëmedia captions, and more. I can also rewrite or edit existing text for tone, clarity, conciseness, or style. |\n",
            "| **Learning & Tutoring** | Explain math problems step‚Äëby‚Äëstep, walk through physics or chemistry concepts, help with language learning, generate practice questions, and provide study strategies. |\n",
            "| **Programming & Technical Help** | Write, debug, and refactor code in many languages (Python, JavaScript, Java, C++, etc.), explain algorithms, suggest data‚Äëstructure choices, create SQL queries, and help with configuration files, APIs, or command‚Äëline tasks. |\n",
            "| **Productivity & Planning** | Build outlines, to‚Äëdo lists, project plans, meeting agendas, timelines, and simple Gantt‚Äëstyle schedules. I can also generate templates for resumes, cover letters, budgets, and business documents. |\n",
            "| **Data & Analysis** | Interpret CSV/JSON snippets, perform basic calculations, generate statistical summaries, suggest visualizations, and explain results in plain language. |\n",
            "| **Language Services** | Translate between many languages, proofread for grammar/spelling, adjust formality level, and localize text for different audiences. |\n",
            "| **Conversation & Role‚ÄëPlay** | Simulate interviews, practice negotiations, act as a mock client or user, provide coaching for public speaking, or just chat for entertainment. |\n",
            "| **Image Understanding (when an image is provided)** | Describe the content, identify objects, read text within the picture (OCR), analyze charts/diagrams, and answer questions about visual details. |\n",
            "| **Ethical Guidance & Safety** | Offer advice on responsible AI use, data privacy, digital well‚Äëbeing, and flag content that may be harmful or disallowed. |\n",
            "\n",
            "### What I **don‚Äôt** do\n",
            "- Access the live internet, real‚Äëtime databases, or personal accounts.\n",
            "- Perform actions in the physical world (e.g., send emails, place orders).\n",
            "- Provide medical, legal, or financial advice that would replace a qualified professional‚Äôs counsel.\n",
            "- Generate disallowed content (e.g., hate speech, extremist propaganda, instructions for illegal activities).\n",
            "\n",
            "### How to get the most out of me\n",
            "1. **Be specific** ‚Äì clear prompts lead to clearer answers.  \n",
            "2. **Provide context** ‚Äì if you have a particular audience, format, or constraint, let me know.  \n",
            "3. **Iterate** ‚Äì you can ask follow‚Äëup questions or request refinements (‚Äúmake it shorter,‚Äù ‚Äúuse a more formal tone,‚Äù etc.).  \n",
            "\n",
            "Feel free to ask for anything within these areas, and I‚Äôll do my best to help!\n",
            "CPU times: user 645 ms, sys: 83 ms, total: 728 ms\n",
            "Wall time: 5.82 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model = MODEL_NAME,\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What are your capabilities?\"\n",
        "    }\n",
        "  ],\n",
        "  temperature=0.6\n",
        ")\n",
        "\n",
        "print ('----model answer -----')\n",
        "print (completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfeT8JxlzaNm"
      },
      "source": [
        "### 3.3 - Ask a factual question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8htBO7iH6-e",
        "outputId": "c8ad3ca3-52b5-4837-9640-992309340ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----model answer -----\n",
            "The capital of France is **Paris**.\n",
            "\n",
            "----- full response ----\n",
            "{\n",
            "  \"id\": \"chatcmpl-2769efe8f3ad49c9a96879ea7439656a\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"The capital of France is **Paris**.\",\n",
            "        \"refusal\": null,\n",
            "        \"role\": \"assistant\",\n",
            "        \"annotations\": null,\n",
            "        \"audio\": null,\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": [],\n",
            "        \"reasoning_content\": \"The user asks: \\\"What is the capital of France?\\\" Straightforward answer: Paris.\"\n",
            "      },\n",
            "      \"stop_reason\": null\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1754544553,\n",
            "  \"model\": \"openai/gpt-oss-120b\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"service_tier\": null,\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 37,\n",
            "    \"prompt_tokens\": 88,\n",
            "    \"total_tokens\": 125,\n",
            "    \"completion_tokens_details\": null,\n",
            "    \"prompt_tokens_details\": null\n",
            "  },\n",
            "  \"prompt_logprobs\": null,\n",
            "  \"kv_transfer_params\": null\n",
            "}\n",
            "---------\n",
            "CPU times: user 6.25 ms, sys: 0 ns, total: 6.25 ms\n",
            "Wall time: 374 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model = MODEL_NAME,\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What is the capital of France?\"\n",
        "    }\n",
        "  ],\n",
        "  temperature=0.6\n",
        ")\n",
        "\n",
        "print ('----model answer -----')\n",
        "print (completion.choices[0].message.content)\n",
        "print ('\\n----- full response ----')\n",
        "print(completion.to_json())\n",
        "print ('---------')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp2JEfLYzgEC"
      },
      "source": [
        "### 3.4 - Ask a reasoning question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5KlDwxJjhEX",
        "outputId": "4a960f44-831f-4160-add0-348c118b4708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is a **step‚Äëby‚Äëstep playbook** you can use (and adapt) the next time you need to shave milliseconds‚Äëto‚Äëseconds off the latency of a model in production.  \n",
            "I‚Äôll walk through the **thought process**, the **key levers** you can pull, and then give **concrete code snippets** for the most common techniques. For each lever I‚Äôll also note **alternatives** and the **trade‚Äëoffs** you‚Äôll have to consider.\n",
            "\n",
            "---\n",
            "\n",
            "## 1Ô∏è‚É£  Start with a Baseline & Profile the Hot Path  \n",
            "\n",
            "> **Why?** You can‚Äôt optimize what you don‚Äôt know is slow.  \n",
            "> **What to measure:**  \n",
            "> - End‚Äëto‚Äëend latency (client ‚Üí API ‚Üí model ‚Üí response)  \n",
            "> - Pure model inference time (exclude network, serialization)  \n",
            "> - CPU/GPU utilization, memory bandwidth, cache‚Äëmiss rates  \n",
            "\n",
            "**Typical tools**\n",
            "\n",
            "| Layer | Tool | What it tells you |\n",
            "|-------|------|-------------------|\n",
            "| Python code | `cProfile`, `line_profiler` | Python‚Äëlevel bottlenecks (data prep, post‚Äëproc) |\n",
            "| TensorFlow | TensorBoard Profiler, `tf.profiler` | Ops breakdown, device placement |\n",
            "| PyTorch | `torch.profiler`, `torch.utils.bottleneck` | Kernel timings, CUDA sync issues |\n",
            "| System | `nvidia‚Äësmi`, `perf`, `vtune` | GPU memory, PCIe throughput, CPU stalls |\n",
            "| End‚Äëto‚Äëend | `wrk`, `hey`, `locust` | Request‚Äëlevel latency, QPS, tail latencies |\n",
            "\n",
            "**Action:** Run a few representative payloads (batch‚Äësize = 1, typical batch size, worst‚Äëcase input) and record the numbers. This gives you a concrete target (e.g., ‚Äúcurrent 95th‚Äëpct latency = 210‚ÄØms, we need <‚ÄØ80‚ÄØms‚Äù).\n",
            "\n",
            "---\n",
            "\n",
            "## 2Ô∏è‚É£  Model‚ÄëLevel Optimizations  \n",
            "\n",
            "These techniques change the *graph* or *weights* of the model. They are usually the first place you get the biggest win with minimal impact on the serving stack.\n",
            "\n",
            "| Technique | What it does | Typical speed‚Äëup | When to use | Alternatives |\n",
            "|-----------|--------------|------------------|-------------|--------------|\n",
            "| **Quantization** (int8, fp16, bfloat16) | Reduces bit‚Äëwidth of weights/activations ‚Üí smaller memory, faster arithmetic on supported HW | 2‚Äë4√ó on GPU/CPU, up to 10√ó on specialized ASICs (e.g., EdgeTPU) | Model is tolerant to reduced precision (most CNNs, Transformers after fine‚Äëtuning) | Dynamic quantization, static quantization, QAT (Quantization‚ÄëAware Training) |\n",
            "| **Pruning / Sparsity** | Zeroes out unimportant weights ‚Üí fewer MACs, can be exploited by sparse kernels | 1.5‚Äë2√ó on hardware that supports sparsity (e.g., NVIDIA Ampere‚Äôs 2:4 structured sparsity) | You have a lot of over‚Äëparameterization, can re‚Äëtrain/fine‚Äëtune after pruning | Structured pruning (filter/channel) vs. unstructured |\n",
            "| **Knowledge Distillation** | Train a small ‚Äústudent‚Äù to mimic a large ‚Äúteacher‚Äù ‚Üí smaller architecture | 2‚Äë10√ó depending on size reduction | Need to preserve accuracy, have data for distillation | Teacher‚Äëfree distillation (self‚Äëdistill), early‚Äëexit models |\n",
            "| **Operator Fusion / Graph Rewrites** | Merge adjacent ops (e.g., Conv+BatchNorm+ReLU) into a single kernel | 1.2‚Äë1.5√ó, reduces memory traffic | Most frameworks already do this, but custom ops may need manual fusion | Use compiler stacks like TVM, TensorRT, XLA |\n",
            "| **Neural Architecture Search (NAS) for latency** | Search for architecture that meets a latency budget on target HW | Up to 5√ó if you start from a generic model | Expensive compute budget, usually done offline | Manual redesign, MobileNet‚Äëstyle depthwise separable convs |\n",
            "\n",
            "### Quick‚ÄëStart Quantization Example (PyTorch ‚Üí ONNX ‚Üí TensorRT)\n",
            "\n",
            "```python\n",
            "# 1Ô∏è‚É£  Load a float32 model (e.g., ResNet50)\n",
            "import torch, torchvision\n",
            "model = torchvision.models.resnet50(pretrained=True).eval()\n",
            "\n",
            "# 2Ô∏è‚É£  Switch to evaluation mode and fuse modules (conv+bn+relu)\n",
            "model_fused = torch.quantization.fuse_modules(\n",
            "    model,\n",
            "    [['conv1', 'bn1', 'relu']],\n",
            "    inplace=True\n",
            ")\n",
            "\n",
            "# 3Ô∏è‚É£  Prepare for static quantization\n",
            "model_fused.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
            "torch.quantization.prepare(model_fused, inplace=True)\n",
            "\n",
            "# 4Ô∏è‚É£  Calibrate with a few batches of real data\n",
            "import torch.utils.data as data\n",
            "calib_loader = data.DataLoader(\n",
            "    torchvision.datasets.ImageNet('./data', split='val',\n",
            "                                  transform=torchvision.transforms.Compose([\n",
            "                                      torchvision.transforms.Resize(256),\n",
            "                                      torchvision.transforms.CenterCrop(224),\n",
            "                                      torchvision.transforms.ToTensor()\n",
            "                                  ])),\n",
            "    batch_size=32, shuffle=False, num_workers=4)\n",
            "\n",
            "with torch.no_grad():\n",
            "    for imgs, _ in calib_loader:\n",
            "        model_fused(imgs)\n",
            "        break   # a few batches are enough for calibration\n",
            "\n",
            "# 5Ô∏è‚É£  Convert to int8\n",
            "torch.quantization.convert(model_fused, inplace=True)\n",
            "\n",
            "# 6Ô∏è‚É£  Export to ONNX (TensorRT can ingest int8 ONNX)\n",
            "dummy = torch.randn(1, 3, 224, 224)\n",
            "torch.onnx.export(model_fused, dummy, \"resnet50_int8.onnx\",\n",
            "                  opset_version=13, input_names=['input'],\n",
            "                  output_names=['output'], dynamic_axes={'input': {0: 'batch'}})\n",
            "\n",
            "# 7Ô∏è‚É£  Build TensorRT engine (CLI example)\n",
            "#   trtexec --onnx=resnet50_int8.onnx --int8 --saveEngine=resnet50_int8.trt\n",
            "```\n",
            "\n",
            "**Why this path?**  \n",
            "- **PyTorch ‚Üí ONNX** is framework‚Äëagnostic, letting you later serve with TensorRT, ONNX Runtime, or any other runtime that supports int8.  \n",
            "- **Static quantization** (calibration) gives the best speed on CPUs/GPUs that have int8 kernels.  \n",
            "- **TensorRT** automatically fuses ops, applies layer‚Äëwise kernel auto‚Äëtuning, and can exploit NVIDIA‚Äôs structured sparsity if you also prune.\n",
            "\n",
            "**Alternatives**  \n",
            "- **Dynamic quantization** (`torch.quantization.quantize_dynamic`) ‚Äì easier, works on CPU only, no calibration needed.  \n",
            "- **Quantization‚ÄëAware Training (QAT)** ‚Äì if static quantization hurts accuracy >‚ÄØ1‚Äë2‚ÄØ%, train with fake‚Äëquant nodes to recover it.  \n",
            "- **BFloat16** ‚Äì on newer GPUs (A100, H100) you can keep fp16‚Äëcompatible kernels but avoid the accuracy drop of int8.\n",
            "\n",
            "---\n",
            "\n",
            "## 3Ô∏è‚É£  Runtime & Hardware Leveraging  \n",
            "\n",
            "Even a perfectly optimized graph won‚Äôt be fast if the runtime or hardware isn‚Äôt tuned.\n",
            "\n",
            "| Lever | What to do | Typical gain | When it matters | Caveats |\n",
            "|------|------------|--------------|-----------------|---------|\n",
            "| **GPU/TPU selection** | Use latest generation (e.g., NVIDIA A100, H100, or G4 for inference) | 1.5‚Äë3√ó over older cards | High QPS workloads, large batch sizes | Cost & availability |\n",
            "| **FP16 / BF16 inference** | Enable mixed‚Äëprecision kernels (`torch.cuda.amp.autocast`, TensorRT `--fp16`) | 2√ó on GPUs with Tensor Cores | Models that tolerate reduced precision | Slight accuracy loss on some layers |\n",
            "| **Batching** | Accumulate multiple requests before sending to the model (dynamic or static batch) | 2‚Äë10√ó throughput, latency may increase slightly | High QPS, latency budget >‚ÄØ10‚ÄØms | Must manage request queuing & timeout |\n",
            "| **Async / Multi‚Äëstream execution** | Use CUDA streams or async inference APIs (e.g., Triton `async_infer`) | Improves GPU utilization, reduces tail latency | Mixed CPU/GPU workloads | Complexity in code |\n",
            "| **TensorRT / ONNX Runtime / TVM** | Deploy with a high‚Äëperformance runtime that does kernel auto‚Äëtuning | 1.5‚Äë3√ó over vanilla PyTorch/TensorFlow | Production services, need low latency | Requires model conversion (ONNX) |\n",
            "| **Edge ASICs (EdgeTPU, NPU, FPGA)** | Compile model to specialized instruction set (e.g., Coral EdgeTPU) | 10‚Äë30√ó for tiny models | On‚Äëdevice inference, power‚Äëconstrained | Model size limits, quantization mandatory |\n",
            "| **CPU vector extensions** | Enable AVX2/AVX‚Äë512, use Intel OpenVINO, MKL‚ÄëDNN | 2‚Äë4√ó on modern CPUs | When GPU not available or cost‚Äësensitive | Need to build with appropriate flags |\n",
            "\n",
            "### Example: Deploying with NVIDIA Triton Inference Server (Batched FP16)\n",
            "\n",
            "```yaml\n",
            "# model_repository/resnet50/1/model.yaml\n",
            "name: \"resnet50\"\n",
            "platform: \"onnxruntime_onnx_fp16\"\n",
            "max_batch_size: 32          # enables static batching up to 32\n",
            "input:\n",
            "  - name: \"input\"\n",
            "    data_type: TYPE_FP16\n",
            "    dims: [3, 224, 224]\n",
            "output:\n",
            "  - name: \"output\"\n",
            "    data_type: TYPE_FP32\n",
            "    dims: [1000]\n",
            "```\n",
            "\n",
            "```bash\n",
            "# 1Ô∏è‚É£  Start Triton (Docker)\n",
            "docker run --gpus all -p8000:8000 -p8001:8001 -p8002:8002 \\\n",
            "  -v$(pwd)/model_repository:/models nvcr.io/nvidia/tritonserver:24.09-py3 tritonserver \\\n",
            "  --model-repository=/models --strict-model-config=true\n",
            "\n",
            "# 2Ô∏è‚É£  Client (Python) ‚Äì asynchronous batched request\n",
            "import tritonclient.http as httpclient\n",
            "import numpy as np, asyncio\n",
            "\n",
            "triton_client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
            "\n",
            "async def async_infer(image_np):\n",
            "    inputs = httpclient.InferInput(\"input\", image_np.shape, \"FP16\")\n",
            "    inputs.set_data_from_numpy(image_np.astype(np.float16))\n",
            "    result = await triton_client.async_infer(\n",
            "        model_name=\"resnet50\",\n",
            "        inputs=[inputs],\n",
            "        request_id=\"req-{}\".format(id(image_np))\n",
            "    )\n",
            "    return result.as_numpy(\"output\")\n",
            "\n",
            "# Example usage with an event loop\n",
            "loop = asyncio.get_event_loop()\n",
            "futures = [async_infer(batch) for batch in my_batch_generator()]\n",
            "outputs = loop.run_until_complete(asyncio.gather(*futures))\n",
            "```\n",
            "\n",
            "**Why Triton?**  \n",
            "- Handles **dynamic batching** out‚Äëof‚Äëthe‚Äëbox (you just set `max_batch_size`).  \n",
            "- Offers **model versioning**, **metrics**, **GPU sharing**, and **multiple runtimes** (TensorRT, ONNX Runtime, PyTorch).  \n",
            "- Works well with **async client libraries**, giving you low tail‚Äëlatency without manual thread pools.\n",
            "\n",
            "**Alternatives**  \n",
            "- **TorchServe** ‚Äì simpler if you‚Äôre all‚ÄëPyTorch and need built‚Äëin model‚Äëversioning.  \n",
            "- **KFServing / Seldon Core** ‚Äì Kubernetes‚Äënative, good for multi‚Äëframework ensembles.  \n",
            "- **Custom Flask/FastAPI + TorchScript** ‚Äì fastest to prototype but you lose batching & auto‚Äëtuning.\n",
            "\n",
            "---\n",
            "\n",
            "## 4Ô∏è‚É£  System‚ÄëLevel & Architectural Tweaks  \n",
            "\n",
            "| Lever | Description | Typical effect | When to apply |\n",
            "|------|-------------|----------------|---------------|\n",
            "| **Cold‚Äëstart mitigation** | Keep a warm pool of workers, pre‚Äëload model into GPU memory | Removes first‚Äërequest spikes (often >‚ÄØ500‚ÄØms) | Low‚Äëtraffic services, serverless |\n",
            "| **Model‚Äësharding / Pipeline parallelism** | Split a huge model across multiple GPUs, or run pre‚Äë/post‚Äëprocessing on CPU | Can keep per‚ÄëGPU latency low while scaling throughput | Very large Transformers (BERT‚Äëlarge, LLMs) |\n",
            "| **Cache repeated inference** | Store embeddings or logits for identical inputs (e.g., recommendation candidate generation) | Near‚Äëzero latency for cache hits | High request duplication, low‚Äëentropy inputs |\n",
            "| **Input preprocessing off‚Äëload** | Use a separate microservice (or GPU) for image resizing, tokenization | Reduces request‚Äëhandler CPU load | When preprocessing dominates latency |\n",
            "| **Network & Serialization** | Use gRPC + protobuf, enable HTTP/2, compress payloads | 10‚Äë30‚ÄØ% latency reduction on the wire | High‚ÄëQPS, large payloads |\n",
            "| **Autoscaling with latency‚Äëbased policies** | Scale out when 95th‚Äëpct latency > target, scale in otherwise | Keeps latency within SLA while controlling cost | Cloud‚Äënative deployments (K8s HPA, Cloud Run) |\n",
            "\n",
            "**Design Decision Example:**  \n",
            "Suppose you have a BERT‚Äëbase model serving 200‚ÄØRPS with a 95th‚Äëpct latency of 140‚ÄØms. After profiling you see:\n",
            "\n",
            "- 30‚ÄØms spent in tokenization (CPU)  \n",
            "- 80‚ÄØms spent in the model (GPU)  \n",
            "- 30‚ÄØms in network/serialization  \n",
            "\n",
            "A good first step: **move tokenization to a separate async service** (e.g., FastAPI with `uvicorn --workers 4`). This can parallelize CPU work with GPU inference and often cuts total latency to ~110‚ÄØms without touching the model itself.\n",
            "\n",
            "---\n",
            "\n",
            "## 5Ô∏è‚É£  Validation & Continuous Monitoring  \n",
            "\n",
            "1. **Regression test** the new pipeline against the baseline (latency, throughput, accuracy).  \n",
            "2. **Canary deploy** the optimized model to a small traffic slice (e.g., 5‚ÄØ%).  \n",
            "3. **Monitor**:  \n",
            "   - `latency_p95`, `latency_p99` (Prometheus metrics from Triton/TFServing)  \n",
            "   - GPU memory/SM utilization (`nvml` exporter)  \n",
            "   - Errors / fallback rate (if you have a fallback to float32)  \n",
            "\n",
            "If the canary meets SLA *and* the accuracy drop is within your tolerance (e.g., <‚ÄØ0.5‚ÄØ% top‚Äë1), promote to production.\n",
            "\n",
            "---\n",
            "\n",
            "## üìã TL;DR Checklist\n",
            "\n",
            "| ‚úÖ | Action | Tool / Code |\n",
            "|---|--------|--------------|\n",
            "| 1 | **Profile** real traffic ‚Üí identify bottleneck | `torch.profiler`, `nvprof`, `wrk` |\n",
            "| 2 | **Quantize** (int8) ‚Üí export ONNX ‚Üí TensorRT engine | PyTorch QAT / static quant, `trtexec` |\n",
            "| 3 | **Prune** (structured) ‚Üí fine‚Äëtune | `torch.nn.utils.prune` |\n",
            "| 4 | **Distill** to a smaller architecture if size matters | `torch.nn.KLDivLoss` + teacher logits |\n",
            "| 5 | **Select runtime** that does kernel auto‚Äëtuning | Triton, TensorRT, ONNX Runtime, TVM |\n",
            "| 6 | **Enable batching** (static or dynamic) | Triton `max_batch_size`, TensorFlow `tf.function` with `batch_size` |\n",
            "| 7 | **Use mixed‚Äëprecision** (FP16/BF16) on Tensor Cores | `torch.cuda.amp.autocast`, TensorRT `--fp16` |\n",
            "| 8 | **Deploy** with warm workers, async client, and proper health checks | Docker + Triton, FastAPI for pre‚Äëproc |\n",
            "| 9 | **Monitor & Canary** ‚Üí roll‚Äëout when metrics meet SLA | Prometheus + Grafana, K8s HPA |\n",
            "|10| **Iterate**: if latency still high, revisit architecture (e.g., MobileNetV3, EfficientNet, or a Transformer with early‚Äëexit) | NAS, manual redesign |\n",
            "\n",
            "---\n",
            "\n",
            "### Final Thought\n",
            "\n",
            "Speed is a **stacked problem**: you gain the most by first fixing the *slowest* layer (often the model itself), then tightening the *runtime* and *system* layers. A disciplined profiling‚Äëfirst approach ensures you don‚Äôt waste engineering effort on a technique that yields only marginal gains for your particular workload.  \n",
            "\n",
            "Feel free to let me know which part of your stack you‚Äôd like to dive deeper into (e.g., ‚Äúshow me how to do quant‚Äëaware training for a Vision Transformer‚Äù or ‚Äúset up dynamic batching with FastAPI + TorchServe‚Äù), and I can provide a more focused code walk‚Äëthrough.\n",
            "----------\n",
            "CPU times: user 8.04 ms, sys: 950 ¬µs, total: 8.99 ms\n",
            "Wall time: 23 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "reasoning_effort = \"high\"  # Options: \"low\", \"medium\", \"high\"\n",
        "\n",
        "# try another model\n",
        "completion = client.chat.completions.create(\n",
        "  model = MODEL_NAME,\n",
        "  messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f\"\"\"\n",
        "        You are an expert in MLOps and interviewing for a job.\n",
        "    Reasoning: {reasoning_effort}\n",
        "    For each task, explain your thinking step by step before showing code.\n",
        "    Justify key design decisions and list alternatives when relevant.\n",
        "        \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"How can we make model inferencing faster?\"\n",
        "    }\n",
        "  ],\n",
        "  temperature=0.6\n",
        ")\n",
        "\n",
        "print (completion.choices[0].message.content)\n",
        "print ('----------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7JaLormtgTR"
      },
      "source": [
        "## 5 - Try Your Queries\n",
        "\n",
        "Go ahead and experiment with your queries.  Here are some to get you started.\n",
        "\n",
        "> Write python code to read a csv file\n",
        "\n",
        "> write a haiku about cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "poXPoPfOzomP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "studio-cookbook-1",
      "language": "python",
      "name": "studio-cookbook-1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
